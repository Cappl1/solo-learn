# Config for Temporal MAE pretraining on CORe50 dataset with ResNet-18 backbone
# Compatible with main_pretrain.py

defaults:
  - _self_
  - augmentations: asymmetric.yaml
  - wandb: private.yaml
  - override hydra/hydra_logging: disabled
  - override hydra/job_logging: disabled

# disable hydra outputs
hydra:
  output_subdir: null
  run:
    dir: .

name: "temporal_mae-core50-resnet18"
method: "temporal_mae"
no_validation: False

backbone:
  name: "vit_small"  # MAE works best with ViT backbones
    
method_kwargs:
  # Standard MAE parameters
  mask_ratio: 0.75
  decoder_embed_dim: 256  # Reduced from 512
  decoder_depth: 4        # Reduced from 8
  decoder_num_heads: 8
  norm_pix_loss: True
  
  # Temporal specific parameters
  delta_max: 15
  cross_attention_layers: 2  # Number of layers with cross-attention for temporal context

momentum:
  base_tau: 0.996
  final_tau: 0.996

# NOTE: For Temporal MAE we need at least 2 views (current frame and future frame)
data:
  dataset: "temporal_core50"
  train_path: "/home/brothen/core50_arr.h5"
  val_path: "/home/brothen/core50_arr.h5"
  format: "h5"
  num_classes: 50
  fraction: -1.0
  num_workers: 4
  num_large_crops: 2  # We need exactly 2 crops for temporal prediction
  num_small_crops: 0
  dataset_kwargs:
    backgrounds: ["s1", "s2", "s4", "s5", "s6", "s8", "s9", "s11"]
    val_backgrounds: ["s3", "s7", "s10"]
    time_window: 15

optimizer:
  name: "adamw"  # AdamW typically works better with MAE
  batch_size: 64
  lr: 1.5e-4  # Lower learning rate for MAE
  classifier_lr: 3e-4
  weight_decay: 0.05  # Higher weight decay for MAE
  no_labels: False

scheduler:
  name: "warmup_cosine"
  warmup_epochs: 10
  warmup_start_lr: 0.0

checkpoint:
  enabled: True
  dir: "trained_models"
  frequency: 1
  keep_prev: True
  every_n_iter: 5000

auto_resume:
  enabled: False

# KNN evaluation callback config
knn_clb:
  enabled: True
  dataset: "core50"
  train_path: "/home/brothen/core50_arr.h5"
  val_path: "/home/brothen/core50_arr.h5"
  num_workers: 4
  batch_size: 64
  k: [10, 20, 50, 100]
  temperature: 0.07
  train_backgrounds: ["s1", "s2", "s4", "s5", "s6", "s8", "s9", "s11"]
  val_backgrounds: ["s3", "s7", "s10"]

# training settings
max_epochs: 100
devices: [0,1,2]
sync_batchnorm: True
accelerator: "gpu"
strategy: "ddp_find_unused_parameters_true"
precision: 32
num_sanity_val_steps: 10
check_val_every_n_epoch: 1
num_nodes: 1

# difficulty metrics callback configuration
difficulty_metrics:
  enabled: True
  dataset_path: "/home/brothen/core50_arr.h5"
  eval_frequency: 1
  num_samples: 1000
  compute_rank_stability: True
  save_metrics: True
  output_dir: "/home/brothen/solo-learn/difficulty_metrics_output/mae"
  dataset_kwargs:
    backgrounds: ["s1", "s2", "s4", "s5", "s6", "s8", "s9", "s11"]
    time_window: 15
  model_type: "mae"  # Specify model type for appropriate metrics computation